# -*- coding: utf-8 -*-
"""mlProjectINT254.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/137wyOGQHAW3V4GTPmiQp8Gi5Dy1obg3h
"""

import pandas as pd
df = pd.read_csv("/content/train.csv")

df.head()

print("SHAPE-")
df.shape

print("INFO-")
df.info()

df.isnull().sum()

df['Age'].fillna(df['Age'].median(),inplace=True)
df['Embarked'].fillna(df['Embarked'].mode(),inplace=True)

df.drop(["Name"], axis=1, inplace=True)

cols=['Sex', 'Ticket', 'Cabin', 'Embarked']
from sklearn.preprocessing import LabelEncoder
le=LabelEncoder()
for i in cols:
  df[i]=le.fit_transform(df[i])
print(df.head())

import matplotlib.pyplot as plt
import seaborn as sns
sns.pairplot(df)
plt.show()

df.info()

import numpy as np
corr=np.corrcoef(df.values.T)
plt.figure(figsize=(15,10))
hm = sns.heatmap(corr,annot=True, linewidth=0.5,fmt=".2f")
plt.show()

df.isnull().sum()

import matplotlib.pyplot as plt
df.hist(figsize=(10, 8), color='orange', edgecolor='red')
plt.tight_layout()
plt.show()

df.drop([ "Ticket", "PassengerId"], axis=1, inplace=True)

df.info()

df.shape

target=df['Survived']
input=df.drop(columns=['Survived'])
input.shape

df.describe()

# from sklearn.preprocessing import StandardScaler
# sc = StandardScaler()
# dataset_std=sc.fit_transform(df)
# dataset_std=pd.DataFrame(dataset_std)
# print(dataset_std.describe())

# dataset_std.head()

# target=dataset_std[0]
# input=dataset_std.drop(columns=[0])
# input.shape

target.shape

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(input, target, test_size=0.15)

x_train.shape

x_test.shape

y_train.unique()

y_train.shape

(y_train==0).sum()

(y_train==1).sum()

(y_train==0).count()
(y_train==1).count()

from sklearn.linear_model import Perceptron
p = Perceptron()
p.fit(x_train,y_train)
train_pred=p.predict(x_train)
test_pred=p.predict(x_test)

from sklearn.metrics import accuracy_score
per_train_acc = accuracy_score(train_pred,y_train)
per_test_acc = accuracy_score(test_pred,y_test)
print("Training Accuracy: ", per_train_acc)
print("Testing Accuracy: ", per_test_acc)

plt.figure(figsize=(6, 4))

x = ['Training Accuracy', 'Testing Accuracy']

y = [per_train_acc, per_test_acc]

plt.bar(x, y, color=['pink', 'skyblue'])

plt.xlabel('Accuracy')
plt.ylabel('Value')
plt.title('Perceptron')
plt.ylim(0, 1)

plt.tight_layout()
plt.show()

from sklearn.metrics import confusion_matrix
import seaborn as sns
print("Confusion Matrix : ")
conf_matrix = confusion_matrix(train_pred, y_train)
print(conf_matrix)
plt.figure(figsize=(6, 6))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Reds", cbar=False)
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

print("Confusion Matrix : ")
conf_matrix_test = confusion_matrix(test_pred, y_test)
print(conf_matrix_test)
plt.figure(figsize=(6, 6))
sns.heatmap(conf_matrix_test, annot=True, fmt="d", cmap="Blues", cbar=False)
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

from sklearn.metrics import precision_score, recall_score, f1_score
precision = precision_score(train_pred, y_train)
recall = recall_score(train_pred, y_train)
f1_score = f1_score(train_pred, y_train)
print("Precision: ", precision)
print("Recall: ", recall)
print("F1 Score: ", f1_score)

metrics = ['Precision', 'Recall', 'F1 Score']

values = [precision, recall, f1_score]

plt.figure(figsize=(8, 6))
plt.bar(metrics, values, color=['skyblue', 'orange', 'green'])
plt.xlabel('Metrics')
plt.ylabel('Value')
plt.title('Precision, Recall, and F1 Score')
plt.ylim(0, 1)
plt.show()

from sklearn.model_selection import GridSearchCV

params=[{'penalty':['l1','l2','elasticnet']}, {'alpha':[0.001,0.01,0.1,1]}, {'eta0':[0.001, 0.01, 0.2, 0.5, 0.8]}]

gs=GridSearchCV(estimator=Perceptron(), param_grid=params)

gs.fit(x_train, y_train)

gs.best_score_

gs.best_params_

from sklearn.linear_model import LogisticRegression

lr=LogisticRegression()

lr.fit(x_train, y_train)

lr_train_results=lr.predict(x_train)
lr_test_results=lr.predict(x_test)
lr_train_acc = accuracy_score(lr_train_results, y_train)
lr_test_acc = accuracy_score(lr_test_results, y_test)
print("Training Accuracy : ", lr_train_acc)
print("Testing Accuracy : ", lr_test_acc)

plt.figure(figsize=(6, 4))

x = ['Training Accuracy', 'Testing Accuracy']

y = [lr_train_acc, lr_test_acc]

plt.bar(x, y, color=['pink', 'skyblue'])

plt.xlabel('Accuracy')
plt.ylabel('Value')
plt.title('Logistic Regression')
plt.ylim(0, 1)

plt.tight_layout()
plt.show()

from sklearn.svm import SVC
svc=SVC()

svc.fit(x_train, y_train)

svc_train_results=svc.predict(x_train)
svc_test_results=svc.predict(x_test)
svc_train_acc = accuracy_score(svc_train_results, y_train)
svc_test_acc = accuracy_score(svc_test_results, y_test)
print("Training Accuracy : ", svc_train_acc)
print("Testing Accuracy : ", svc_test_acc)

plt.figure(figsize=(6, 4))

x = ['Training Accuracy', 'Testing Accuracy']

y = [svc_train_acc, svc_test_acc]

plt.bar(x, y, color=['skyblue', 'orange'])

plt.xlabel('Accuracy')
plt.ylabel('Value')
plt.title('Support Vector Classifier')
plt.ylim(0, 1)

plt.tight_layout()
plt.show()

from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier()
knn.fit(x_train,y_train)

knn_train_results = knn.predict(x_train)
knn_test_results = knn.predict(x_test)
knn_train_acc = accuracy_score(knn_train_results, y_train)
knn_test_acc = accuracy_score(knn_test_results, y_test)
print("Training Accuracy : ", knn_train_acc)
print("Testing Accuracy : ", knn_test_acc)

plt.figure(figsize=(6, 4))

x = ['Training Accuracy', 'Testing Accuracy']

y = [knn_train_acc, knn_test_acc]

plt.bar(x, y, color=['pink', 'skyblue'])

plt.xlabel('Accuracy')
plt.ylabel('Value')
plt.title('K Nearest Neighbour')
plt.ylim(0, 1)

plt.tight_layout()
plt.show()

knn_params = [{'n_neighbors' : [1,2,3,4,5,6]}]
knn_gs = GridSearchCV(estimator=KNeighborsClassifier(), param_grid=knn_params)
knn_gs.fit(x_train, y_train)

knn_gs.best_score_

from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier(criterion='entropy',max_depth=6)
dt.fit(x_train, y_train)

dt_train_results = dt.predict(x_train)
dt_test_results = dt.predict(x_test)
dt_train_acc = accuracy_score(dt_train_results, y_train)
dt_test_acc = accuracy_score(dt_test_results, y_test)
print("Training Accuracy : ", dt_train_acc)
print("Testing Accuracy : ", dt_test_acc)

plt.figure(figsize=(6, 4))

x = ['Training Accuracy', 'Testing Accuracy']

y = [dt_train_acc, dt_test_acc]

plt.bar(x, y, color=['pink', 'skyblue'])

plt.xlabel('Accuracy')
plt.ylabel('Value')
plt.title('Decision Tree Classifier')
plt.ylim(0, 1)

plt.tight_layout()
plt.show()

from sklearn.naive_bayes import GaussianNB
nb = GaussianNB()
nb.fit(x_train, y_train)

nb_train_results=nb.predict(x_train)
nb_test_results=nb.predict(x_test)
nb_train_acc = accuracy_score(nb_train_results, y_train)
nb_test_acc = accuracy_score(nb_test_results, y_test)
print("Training Accuracy : ", nb_train_acc)
print("Testing Accuracy : ", nb_test_acc)

plt.figure(figsize=(6, 4))

x = ['Training Accuracy', 'Testing Accuracy']

y = [nb_train_acc, nb_test_acc]

plt.bar(x, y, color=['skyblue', 'orange'])

plt.xlabel('Accuracy')
plt.ylabel('Value')
plt.title('Gaussian Naive Bayes')
plt.ylim(0, 1)

plt.tight_layout()
plt.show()

from sklearn.ensemble import RandomForestClassifier

rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)
rf_classifier.fit(x_train, y_train)

rf_train_results=rf_classifier.predict(x_train)
rf_test_results=rf_classifier.predict(x_test)
rf_train_acc = accuracy_score(rf_train_results, y_train)
rf_test_acc = accuracy_score(rf_test_results, y_test)
print("Training Accuracy : ", rf_train_acc)
print("Testing Accuracy : ", rf_test_acc)

plt.figure(figsize=(6, 4))

x = ['Training Accuracy', 'Testing Accuracy']

y = [rf_train_acc, rf_test_acc]

plt.bar(x, y, color=['skyblue', 'orange'])

plt.xlabel('Accuracy')
plt.ylabel('Value')
plt.title('Random Forest')
plt.ylim(0, 1)

plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
from sklearn.linear_model import Perceptron, LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

models = {
    #"Perceptron": Perceptron(),
    "Logistic Regression": LogisticRegression(),
    #"Suport Vector Machine": SVC(),
    "KNN": KNeighborsClassifier(),
    "Decision Tree": DecisionTreeClassifier(criterion='entropy', max_depth=6),
    #"GaussianNB": GaussianNB(),
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42)
}

accuracies = {"Training": [], "Testing": []}

for model_name, model in models.items():
    model.fit(x_train, y_train)
    train_pred = model.predict(x_train)
    test_pred = model.predict(x_test)
    train_accuracy = accuracy_score(train_pred, y_train)
    test_accuracy = accuracy_score(test_pred, y_test)
    accuracies["Training"].append(train_accuracy)
    accuracies["Testing"].append(test_accuracy)
    print(f"{model_name} Training Accuracy: {train_accuracy}")
    print(f"{model_name} Testing Accuracy: {test_accuracy}")

# Plotting the accuracies
plt.figure(figsize=(10, 6))
for dataset, acc in accuracies.items():
    plt.plot(list(models.keys()), acc, marker='o', label=f"{dataset} Accuracy")

plt.title('Training and Testing Accuracy for Different Classifiers')
plt.xlabel('Model')
plt.ylabel('Accuracy')
plt.ylim(0, 1)
plt.legend()
plt.grid(True)
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

